{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation using real-valued computational graph \n",
    "\n",
    "```\n",
    " dQ/dX\n",
    "X ---\\\n",
    "      \\    df/dQ\n",
    "       (+)---\\\n",
    " dQ/dY/   Q   \\\n",
    "Y ---/         \\\n",
    "                (*)--- f(x, y, z)\n",
    "               /   df/df\n",
    " df/dZ        /\n",
    "Z -----------/\n",
    "```\n",
    "\n",
    "#### By Chain rule\n",
    "```\n",
    "given Q = X + Y\n",
    "\n",
    "df/dQ = df/df * df/dQ\n",
    "df/dX = df/dQ * dQ/dX\n",
    "df/dY = df/dQ * dQ/dY\n",
    "df/dZ = df/df * df/dZ\n",
    "```\n",
    "by manually calculating the partial derivatives...\n",
    "```\n",
    "df/dQ = Z\n",
    "df/dX = Z * 1\n",
    "df/dY = Z * 1\n",
    "df/dZ = X + Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y, z):\n",
    "    return z*(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x, y, z):\n",
    "    dfdx = z\n",
    "    dfdy = z\n",
    "    dfdz = x + y\n",
    "    return dfdx, dfdy, dfdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "y = 5\n",
    "z = 9\n",
    "dx, dy, dz = backward(x, y, z)\n",
    "print (\"Gradients on X: {}, Y: {} and Z: {}\".format(dx, dy, dz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Converting the above computation graph to an optimization problem\n",
    "\n",
    "### This can simply be done by adding a loss layer at the end. Lets say we add a mean square loss `node` after `f(x, y, z)`. Here we define value `V` that we want to optimize using the function `f(x, y, z)`. Below is how the new computational graph would look like. Notice that we are taking derivatives w.r.t `L` not `f` anymore.\n",
    "\n",
    "```\n",
    " dQ/dX\n",
    "X ---\\\n",
    "      \\    df/dQ\n",
    "       (+)---\\\n",
    " dQ/dY/   Q   \\\n",
    "Y ---/         \\\n",
    "                (*)--- f(x, y, z)---Loss\n",
    "               /   dL/df\n",
    "        df/dZ /\n",
    "Z -----------/\n",
    "```\n",
    "\n",
    "```\n",
    "L = (V - f(x,y,z))^2\n",
    "dL/dQ = dL/df * df/dQ\n",
    "dL/dX = dL/dQ * dQ/dX\n",
    "dL/dY = dL/dQ * dQ/dY\n",
    "dL/dZ = dL/df * df/dZ\n",
    "```\n",
    "\n",
    "analytically this becomes...\n",
    "\n",
    "```\n",
    "dL/dQ = -2(V-f)*Z\n",
    "dL/dX = -2(V-f)*Z*1 = dL/dQ*Z\n",
    "dL/dY = -2(V-f)*Z*1 = dL/dQ*Z\n",
    "dL/dZ = -2(V-f)*(X + Y)\n",
    "\n",
    "```\n",
    "\n",
    "now the backward calculation changes to below. You can verify this using wolframalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_w_loss(V, x, y, z):\n",
    "    dldx = -2 * z * (V - (x + y) * z)\n",
    "    dldy = -2 * z * (V - (x + y) * z)\n",
    "    dldz = -2 * (x + y) * (V - (x + y) * z)\n",
    "    return dldx, dldy, dldz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop for variables `x`, `y` and `z`\n",
    "\n",
    "Here I am optimizing the variables to produce a `final_value` of `144`. Which is partly equivalent to training a neural network to learn a certain output/final_value. Notice how in the iteration, you do not need the `forward` function (only in this case because it is a simple/small network and we are solving the entire network analytically). If you were do this piecewise as in a `real` neural network, you need to do the `forward` pass to calculate the loss, which will then get backpropogated through the network.\n",
    "\n",
    "Play around with all initial values of the variables (long enough) and you will essentially find all the real values of `x`, `y` and `z` that satisfy the equation. \n",
    "\n",
    "Notice that this problem has the same steps as the previous curvefitting assigment. Here instead you are training a computational graph to approximate an output value. Also a more complex computational graph like a neural network will follow the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 12.3791\n",
    "y = 1.4782\n",
    "z = 8.192\n",
    "alpha = 0.001\n",
    "final_value = 144\n",
    "n = 1000\n",
    "for i in range(n):\n",
    "    dx, dy, dz = backward_w_loss(final_value, x, y, z)\n",
    "    x = x - alpha * dx\n",
    "    y = y - alpha * dy\n",
    "    z = z - alpha * dz\n",
    "print (\"Final Values of x: {}, y: {} and z: {}\".format(x, y, z))\n",
    "# forward is only used here below\n",
    "print (\"Evaluation of (x + y) * z = {}\".format(forward(x, y, z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Design another computational graph where you introduce weights in the intermediate nodes.\n",
    "\n",
    "- Write a function to calculate the backward pass to optimize only the weights in the computational graph. Keep the `final_value` and your inputs constant. NOTE: this design of the computational graph would be more in the flavor of a neural network.\n",
    "- Optimize the computational graph to obtain optimal weights such that your input variables produce your `final_value`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
