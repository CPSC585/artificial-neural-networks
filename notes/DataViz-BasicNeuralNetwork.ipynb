{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization and manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from numpy import set_printoptions\n",
    "set_printoptions(precision=3)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (16,12)\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load diabetes data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/pima-indians-diabetes.data.csv\"\n",
    "columns = ['pregnant', 'plasma_glucose', 'blood_pressure', 'skin_fold', 'serum_insulin', \n",
    "           'bmi', 'pedigree', 'age', 'class']\n",
    "data = pd.read_csv(filename, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head())\n",
    "display(data.sample(5, random_state=1))\n",
    "print \"Data Rows: {}, Cols: {}\".format(data.shape[0], data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  `groupby` operation to consolidate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.groupby('class').sum())\n",
    "display(data.groupby('class').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `apply` to manipulate columns\n",
    "\n",
    "You can pass in any `function`, Return type depends on whether passed function aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.apply(np.mean))\n",
    "display(data.apply(np.sin).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr(method='pearson')\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr, cmap=sns.cubehelix_palette(as_cmap=True), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Skew\n",
    "sns.distplot(data['pedigree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Skew\n",
    "sns.distplot(data['blood_pressure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[['plasma_glucose', 'blood_pressure', 'serum_insulin', 'class']], hue=\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test = SelectKBest(score_func=chi2, k=3)\n",
    "fit = test.fit(X, Y)\n",
    "print(fit.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression() # Model does not matter much\n",
    "# Select top 5 features\n",
    "rfe = RFE(model, 5)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance using Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extratrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "# model = LogisticRegression()\n",
    "model = ExtraTreesClassifier(max_depth=100)\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (result*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss or Negative Log Likelihood\n",
    "\n",
    "$$-\\frac{1}{N}\\sum_{n \\epsilon N}\\sum_{i \\epsilon C} y_{n,i} log \\hat{y}_{n,i}$$\n",
    "\n",
    "\n",
    "### Gradients\n",
    "\n",
    "---\n",
    "\n",
    "$$\\delta_{2} = \\hat{y} - y$$\n",
    "\n",
    "$$\\delta_{1} = \\delta_{2} W_{2}^{T}\\odot a_{1} \\odot (1 - a_{1})$$\n",
    "\n",
    "---\n",
    "\n",
    "$$ \\frac{\\delta L}{\\delta W_{2}} = a^{T}_{1} \\delta_{2}$$\n",
    "\n",
    "$$ \\frac{\\delta L}{\\delta b_{2}} = \\delta_{2}$$\n",
    "\n",
    "$$ \\frac{\\delta L}{\\delta W_{1}} = x^{T} \\delta_{1}$$\n",
    "\n",
    "$$ \\frac{\\delta L}{\\delta b_{2}} = \\delta_{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_features=10, n_output=10, n_hidden=100, \n",
    "                 learning_rate=0.001, reg_lambda=None):\n",
    "        \n",
    "        ### Network Dimensions\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        ### Initialize weights\n",
    "        self.w_hid = np.random.randn(self.n_features, self.n_hidden)\n",
    "        self.b_hid = np.random.randn(self.n_hidden)\n",
    "        self.w_out = np.random.randn(self.n_hidden, self.n_output)\n",
    "        self.b_out = np.random.randn(self.n_output)\n",
    "        \n",
    "        ### Hyper parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "    def _one_hotize(self, y, k):\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def train(self, input_list, target_list):\n",
    "        ### Convert inputs list to 2d array\n",
    "        inputs = np.array(input_list, ndmin=2)\n",
    "        targets = np.array(target_list, ndmin=2)\n",
    "        targets = self._one_hotize(targets, self.n_output).T\n",
    "        \n",
    "        ### Forward \n",
    "        A = self._sigmoid(inputs.dot(self.w_hid) + self.b_hid)\n",
    "        Y = self._softmax(A.dot(self.w_out) + self.b_out)\n",
    "        \n",
    "        ### Calculate Loss ###\n",
    "        loss = np.sum(targets * np.log(Y))/len(targets)\n",
    "        \n",
    "        ### Backward\n",
    "        # Total Error\n",
    "        delta2 = Y - targets\n",
    "        delta1 = delta2.dot(self.w_out.T) * A * (1 - A)\n",
    "        \n",
    "        dw_out = A.T.dot(delta2)\n",
    "        db_out = delta2.sum(axis=0)\n",
    "        dw_hid = inputs.T.dot(delta1)\n",
    "        db_hid = delta1.sum(axis=0)\n",
    "        \n",
    "        ### Add L2 regularization terms (b1 and b2 don't have regularization terms)\n",
    "        if reg_lambda:\n",
    "            dw_out += self.reg_lambda * self.w_out\n",
    "            dw_hid += self.reg_lambda * self.w_hid\n",
    "        \n",
    "        ### Update Weights\n",
    "        self.w_hid -= self.learning_rate * dw_hid\n",
    "        self.b_hid -= self.learning_rate * db_hid\n",
    "        self.w_out -= self.learning_rate * dw_out\n",
    "        self.b_out -= self.learning_rate * db_out\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def inference(self, input_list):\n",
    "        inputs = np.array(input_list, ndmin=2)\n",
    "        A = self._sigmoid(inputs.dot(self.w_hid) + self.b_hid)\n",
    "        Y = self._softmax(A.dot(self.w_out) + self.b_out)\n",
    "        return Y\n",
    "    \n",
    "    def get_accuracy(self, input_list, target_labels):\n",
    "        preds = np.argmax(self.inference(input_list), axis=1)\n",
    "        accuracy = np.sum(target_labels == preds, axis=0) * 1.0/len(target_labels)\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data ###\n",
    "filename = \"../data/pima-indians-diabetes.data.csv\"\n",
    "columns = ['pregnant', 'plasma_glucose', 'blood_pressure', 'skin_fold', 'serum_insulin', \n",
    "           'bmi', 'pedigree', 'age', 'class']\n",
    "data = pd.read_csv(filename, names=columns)\n",
    "\n",
    "#### Normalize Data ###\n",
    "feature_columns = ['pregnant', 'plasma_glucose', 'blood_pressure', 'skin_fold', \n",
    "                   'serum_insulin', 'bmi', 'pedigree', 'age']\n",
    "scaled_features = {}\n",
    "for col in feature_columns:\n",
    "    mean, std = data[col].mean(), data[col].std()\n",
    "    scaled_features[col] = [mean, std]\n",
    "    data.loc[:, col] = (data[col] - mean)/std\n",
    "\n",
    "#### Drop Less \"Important\" columns ###\n",
    "data = data.drop(['pregnant', 'blood_pressure', 'skin_fold', \n",
    "                  'serum_insulin', 'pedigree'], axis=1)\n",
    "    \n",
    "### Train/Test Split ###\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "epochs = 1000\n",
    "n_features = X_train.shape[1]\n",
    "n_output = 2\n",
    "n_hidden = 100\n",
    "learning_rate = 0.001\n",
    "reg_lambda = 0.001\n",
    "\n",
    "### Stats ###\n",
    "training_stats = {'training_acc': [], 'validation_acc': [], 'loss': []}\n",
    "\n",
    "network = NeuralNetwork(n_features, n_output, n_hidden, learning_rate, reg_lambda)\n",
    "for e in range(epochs):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(X_train.index, size=128)\n",
    "    loss = 0\n",
    "    n = 0\n",
    "    for record, target in zip(X_train.ix[batch].values, y_train.ix[batch]['class']):\n",
    "        loss += network.train(record, target)\n",
    "        n += 1\n",
    "    loss = loss/n\n",
    "    training_stats['loss'].append(loss)\n",
    "\n",
    "    # Printing out the training progress\n",
    "    train_acc = network.get_accuracy(X_train, list(y_train['class']))\n",
    "    val_acc = network.get_accuracy(X_test, list(y_test['class']))\n",
    "    sys.stdout.write(\"\\rProgress: \" + str(100 * e/float(epochs))[:4] \\\n",
    "                     + \"% ... Training acc: {}%\".format(str(train_acc * 100)[:5]) \\\n",
    "                     + \" ... Validation acc: {}%\".format(str(val_acc * 100)[:5]))\n",
    "    training_stats['training_acc'].append(train_acc)\n",
    "    training_stats['validation_acc'].append(val_acc)\n",
    "print \"\\n-------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame.from_dict(training_stats)\n",
    "display(stats_df.head())\n",
    "stats_df[['training_acc', 'validation_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotize(df):\n",
    "    onehot = []\n",
    "    for row in df.iterrows():\n",
    "        cod = [0, 0]\n",
    "        cod[row[1]['class']] = 1\n",
    "        onehot.append(cod)\n",
    "    onehot = np.array(onehot)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/pima-indians-diabetes.data.csv\"\n",
    "columns = ['pregnant', 'plasma_glucose', 'blood_pressure', 'skin_fold', 'serum_insulin', \n",
    "           'bmi', 'pedigree', 'age', 'class']\n",
    "data = pd.read_csv(filename, names=columns)\n",
    "\n",
    "#### Normalize Data ###\n",
    "feature_columns = ['pregnant', 'plasma_glucose', 'blood_pressure', 'skin_fold', \n",
    "                   'serum_insulin', 'bmi', 'pedigree', 'age']\n",
    "scaled_features = {}\n",
    "for col in feature_columns:\n",
    "    mean, std = data[col].mean(), data[col].std()\n",
    "    scaled_features[col] = [mean, std]\n",
    "    data.loc[:, col] = (data[col] - mean)/std\n",
    "\n",
    "#### Drop Less \"Important\" columns ###\n",
    "data = data.drop(['skin_fold', 'serum_insulin', 'age'], axis=1)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "Y = data.iloc[:,-1:]\n",
    "Y_onehot = one_hotize(Y)\n",
    "n_features = X.shape[1]\n",
    "n_output = 2\n",
    "n_hidden = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_features, init='uniform', activation='relu'))\n",
    "model.add(Dense(n_hidden, init='uniform', activation='relu'))\n",
    "model.add(Dense(n_output, init='uniform', activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "training = model.fit(X, Y_onehot, validation_split=0.20, epochs=100, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(training.history)[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
